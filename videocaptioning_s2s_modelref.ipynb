{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "#import ipdb\n",
    "import time\n",
    "import cv2\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import fire\n",
    "from elapsedtimer import ElapsedTimer\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import gc;gc.collect()\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "#print('tensorflow version:',tf.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoCaptioning:\n",
    "    \n",
    "    \n",
    "    def __init__(self,path_prj,caption_file,feat_dir,\n",
    "                 cnn_feat_dim=2048,h_dim=512,\n",
    "                 lstm_steps=80,video_steps=80,\n",
    "                 out_steps=20, frame_step=80,\n",
    "                 batch_size=1,learning_rate=1e-4,\n",
    "                 epochs=1,model_path=None,\n",
    "                 mode='test'):\n",
    "\n",
    "        self.dim_image = cnn_feat_dim\n",
    "        self.dim_hidden = h_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm_steps = lstm_steps\n",
    "        self.video_lstm_step=video_steps\n",
    "        self.caption_lstm_step=out_steps\n",
    "        self.path_prj = Path(path_prj)\n",
    "        self.mode = mode\n",
    "        if mode == 'train':\n",
    "            self.train_text_path = self.path_prj / caption_file\n",
    "            self.train_feat_path = self.path_prj / feat_dir\n",
    "        else:\n",
    "            self.test_text_path = self.path_prj / caption_file\n",
    "            self.test_feat_path = self.path_prj / feat_dir\n",
    "        #self.test_text_path = self.path_prj / test_file\n",
    "        #self.test_feat_path = self.path_prj / feat_dir    \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.frame_step = frame_step\n",
    "        self.model_path = model_path\n",
    "\n",
    "    def build_model(self):\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        #tf.Graph()\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "        # Defining the weights associated with the Network\n",
    "        with tf.device('/cpu:0'): \n",
    "            #print(self.n_words)\n",
    "            #print(self.dim_hidden)\n",
    "            #return\n",
    "            #random_tf=tf.random_uniform([8423,512], -0.1, 0.1)\n",
    "            #self.word_emb=tf.Variable(random_tf)\n",
    "            self.word_emb = tf.Variable(tf.random.uniform([self.n_words, self.dim_hidden], -0.1, 0.1), name='word_emb')\n",
    "            print(\"word_emb\",self.word_emb)\n",
    "\n",
    "        \n",
    "       \n",
    "        self.lstm1 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(self.dim_hidden, state_is_tuple=False)\n",
    "        self.lstm2 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(self.dim_hidden, state_is_tuple=False)\n",
    "        self.encode_W = tf.Variable( tf.random.uniform([self.dim_image,self.dim_hidden], -0.1, 0.1), name='encode_W')\n",
    "        self.encode_b = tf.Variable( tf.zeros([self.dim_hidden]), name='encode_b')\n",
    "        \n",
    "        self.word_emb_W = tf.Variable(tf.random.uniform([self.dim_hidden,self.n_words], -0.1,0.1), name='word_emb_W')\n",
    "        self.word_emb_b = tf.Variable(tf.zeros([self.n_words]), name='word_emb_b')\n",
    "        \n",
    "        # Placeholders \n",
    "        video = tf.compat.v1.placeholder(tf.float32, [self.batch_size, self.video_lstm_step, self.dim_image])\n",
    "        video_mask = tf.compat.v1.placeholder(tf.float32, [self.batch_size, self.video_lstm_step])\n",
    "\n",
    "        caption = tf.compat.v1.placeholder(tf.int32, [self.batch_size, self.caption_lstm_step+1])\n",
    "        caption_mask = tf.compat.v1.placeholder(tf.float32, [self.batch_size, self.caption_lstm_step+1])\n",
    "\n",
    "        video_flat = tf.reshape(video, [-1, self.dim_image])\n",
    "        print(\"video_flat\",video_flat)\n",
    "        image_emb = tf.compat.v1.nn.xw_plus_b( video_flat, self.encode_W,self.encode_b ) \n",
    "        print(\"image_emb\",image_emb)\n",
    "        #using image embedding to reduce the dimension to 512\n",
    "        image_emb = tf.reshape(image_emb, [self.batch_size, self.lstm_steps, self.dim_hidden])\n",
    "        print(\"image_emb_reshaping\",image_emb) \n",
    "        state1 = tf.zeros([self.batch_size, self.lstm1.state_size])\n",
    "        state2 = tf.zeros([self.batch_size, self.lstm2.state_size])\n",
    "        padding = tf.zeros([self.batch_size, self.dim_hidden])\n",
    "        print(self.lstm1.state_size) \n",
    "        print(self.lstm2.state_size) \n",
    "        probs = []\n",
    "        loss = 0.0\n",
    "\n",
    "        #  Encoding Stage \n",
    "        for i in range(0, self.video_lstm_step):\n",
    "            if i > 0:\n",
    "                tf.compat.v1.get_variable_scope().reuse_variables()\n",
    "\n",
    "            with tf.compat.v1.variable_scope(\"LSTM1\"):\n",
    "                output1, state1 = self.lstm1(image_emb[:,i,:], state1)\n",
    "                print(\"encoding output1 state1\", output1,state1)\n",
    "            with tf.compat.v1.variable_scope(\"LSTM2\"):\n",
    "                output2, state2 = self.lstm2(tf.concat([padding, output1],1), state2)\n",
    "                print(\"encoding output2 state2\", output2,state2)\n",
    "        #  Decoding Stage  to generate Captions \n",
    "        for i in range(0, self.caption_lstm_step):\n",
    "            print(\"iteration:\",i)\n",
    "            with tf.device(\"/cpu:0\"):# looks for the id's from word embedding\n",
    "                current_embed = tf.compat.v1.nn.embedding_lookup(self.word_emb, caption[:, i])\n",
    "\n",
    "            tf.compat.v1.get_variable_scope().reuse_variables()\n",
    "\n",
    "            with tf.compat.v1.variable_scope(\"LSTM1\"):\n",
    "                print(\"decoding input state1 from previous loop\", output1,state1)\n",
    "\n",
    "                output1, state1 = self.lstm1(padding, state1)\n",
    "                print(\"decoding output1 state1\", output1,state1)   \n",
    "            with tf.compat.v1.variable_scope(\"LSTM2\"):\n",
    "                output2, state2 = self.lstm2(tf.concat([current_embed, output1],1), state2)\n",
    "                print(\"decoding output2 state2\", output2,state2) \n",
    "                print(\"current_embed:\",current_embed)\n",
    "            labels = tf.expand_dims(caption[:, i+1], 1)\n",
    "            print(\"labels:\",labels)\n",
    "            print(\"caption:\",caption)\n",
    "            indices = tf.expand_dims(tf.range(0, self.batch_size, 1), 1)\n",
    "            print(\"indices:\",indices)\n",
    "            concated = tf.concat([indices, labels],1)\n",
    "            onehot_labels = tf.compat.v1.sparse_to_dense(concated, tf.stack([self.batch_size, self.n_words]), 1.0, 0.0)\n",
    "            print(\"onehot_labels:\",onehot_labels)\n",
    "            logit_words = tf.compat.v1.nn.xw_plus_b(output2, self.word_emb_W, self.word_emb_b)\n",
    "            print(logit_words)      \n",
    "        # Computing the loss     \n",
    "            cross_entropy = tf.compat.v1.nn.softmax_cross_entropy_with_logits(labels=onehot_labels,logits=logit_words)\n",
    "            cross_entropy = cross_entropy * caption_mask[:,i]\n",
    "            probs.append(logit_words)\n",
    "            print(logit_words)\n",
    "            print(output2)\n",
    "            print(probs)   \n",
    "            current_loss = tf.reduce_sum(cross_entropy)/self.batch_size\n",
    "            print(\"current_loss\",current_loss)\n",
    "            loss = loss + current_loss\n",
    "        with tf.compat.v1.variable_scope(tf.compat.v1.get_variable_scope(),reuse=tf.compat.v1.AUTO_REUSE) as scope:\n",
    "            train_op = tf.compat.v1.train.AdamOptimizer(self.learning_rate).minimize(loss)    \n",
    "        #allops=tf_graph. get_operations()\n",
    "        #print(allops)\n",
    "        #return\n",
    "        return loss,video,video_mask,caption,caption_mask,probs,train_op\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "        #tf.Graph()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "        with tf.device('/cpu:0'):\n",
    "            self.word_emb = tf.Variable(tf.random.uniform([self.n_words, self.dim_hidden], -0.1, 0.1), name='word_emb')\n",
    "\n",
    "\n",
    "        self.lstm1 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(self.dim_hidden, state_is_tuple=False)\n",
    "        self.lstm2 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(self.dim_hidden, state_is_tuple=False)\n",
    "\n",
    "        self.encode_W = tf.Variable( tf.random.uniform([self.dim_image,self.dim_hidden], -0.1, 0.1), name='encode_W')\n",
    "        self.encode_b = tf.Variable( tf.zeros([self.dim_hidden]), name='encode_b')\n",
    "\n",
    "        self.word_emb_W = tf.Variable(tf.random.uniform([self.dim_hidden,self.n_words], -0.1,0.1), name='word_emb_W')\n",
    "        self.word_emb_b = tf.Variable(tf.zeros([self.n_words]), name='word_emb_b')\n",
    "        video = tf.compat.v1.placeholder(tf.float32, [1, self.video_lstm_step, self.dim_image])\n",
    "        video_mask = tf.compat.v1.placeholder(tf.float32, [1, self.video_lstm_step])\n",
    "\n",
    "        video_flat = tf.reshape(video, [-1, self.dim_image])\n",
    "        image_emb = tf.compat.v1.nn.xw_plus_b(video_flat, self.encode_W, self.encode_b)\n",
    "        image_emb = tf.reshape(image_emb, [1, self.video_lstm_step, self.dim_hidden])\n",
    "\n",
    "        state1 = tf.zeros([1, self.lstm1.state_size])\n",
    "        state2 = tf.zeros([1, self.lstm2.state_size])\n",
    "        padding = tf.zeros([1, self.dim_hidden])\n",
    "\n",
    "        generated_words = []\n",
    "\n",
    "        probs = []\n",
    "        embeds = []\n",
    "\n",
    "        for i in range(0, self.video_lstm_step):\n",
    "            if i > 0:\n",
    "                tf.compat.v1.get_variable_scope().reuse_variables()\n",
    "\n",
    "            with tf.compat.v1.variable_scope(\"LSTM1\"):#,reuse=tf.compat.v1.AUTO_REUSE) as scope:\n",
    "                output1, state1 = self.lstm1(image_emb[:, i, :], state1)\n",
    "\n",
    "            with tf.compat.v1.variable_scope(\"LSTM2\"):#,reuse=tf.compat.v1.AUTO_REUSE) as scope:\n",
    "                output2, state2 = self.lstm2(tf.concat([padding, output1],1), state2)\n",
    "\n",
    "        for i in range(0, self.caption_lstm_step):\n",
    "            tf.compat.v1.get_variable_scope().reuse_variables()\n",
    "\n",
    "            if i == 0:\n",
    "                with tf.device('/cpu:0'):\n",
    "                    current_embed = tf.compat.v1.nn.embedding_lookup(self.word_emb, tf.ones([1], dtype=tf.int64))\n",
    "\n",
    "            with tf.compat.v1.variable_scope(\"LSTM1\"):\n",
    "                output1, state1 = self.lstm1(padding, state1)\n",
    "\n",
    "            with tf.compat.v1.variable_scope(\"LSTM2\"):\n",
    "                output2, state2 = self.lstm2(tf.concat([current_embed, output1],1), state2)\n",
    "\n",
    "            logit_words = tf.compat.v1.nn.xw_plus_b(output2, self.word_emb_W, self.word_emb_b)\n",
    "            max_prob_index = tf.argmax(logit_words, 1)[0]\n",
    "            generated_words.append(max_prob_index)\n",
    "            probs.append(logit_words)\n",
    "            #print(\"generated_words:\",generated_words)\n",
    "\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                current_embed = tf.compat.v1.nn.embedding_lookup(self.word_emb, max_prob_index)\n",
    "                current_embed = tf.expand_dims(current_embed, 0)\n",
    "\n",
    "            embeds.append(current_embed)\n",
    "        #var_list=[v.name for v in tf.compat.v1.get_variable(self.encode_W)]\n",
    "        #print(var_list)\n",
    "        return video,video_mask,generated_words,probs,embeds\n",
    "\n",
    "\n",
    "    def get_data(self,text_path,feat_path):\n",
    "        text_data = pd.read_csv(text_path, sep=',',engine='python')#read the csv into a dataframe\n",
    "        text_data = text_data[text_data['Language'] == 'English']# filter for english\n",
    "        #print(text_data.shape)\n",
    "        #add a column contanating videoid,start and end time to match the video file names\n",
    "        text_data['video_path'] = text_data.apply(lambda row: row['VideoID']+'_'+str(int(row['Start']))+'_'+str(int(row['End']))+'.npy', axis=1)\n",
    "        #print(\"first\",text_data['video_path'] )\n",
    "        # use the column name above and add the path or each videopath name\n",
    "        text_data['video_path'] = text_data['video_path'].map(lambda x: os.path.join(feat_path, x))\n",
    "        #print(\"second\",text_data['video_path'] )\n",
    "\n",
    "        #print(text_data.shape)\n",
    "        text_data.to_csv(r'video_captioning_dataset\\YouTubeClips\\getdata_textdata.csv')\n",
    "        \n",
    "        text_data = text_data[text_data['video_path'].map(lambda x: os.path.exists(x))]#filter by video path\n",
    "        #print(\"mapping\",text_data)\n",
    "        text_data = text_data[text_data['Description'].map(lambda x: isinstance(x, str))]#filter description as string\n",
    "        #print(\"description\",text_data)\n",
    "        unique_filenames=sorted(text_data['video_path'].unique())\n",
    "        #print(\"unique_filenames\",unique_filenames)\n",
    "        #text_data = text_data[text_data['video_path'].map(lambda x: x in unique_filenames)]\n",
    "        data = text_data[text_data['video_path'].map(lambda x: x in unique_filenames)]\n",
    "        #print(data)\n",
    "        return data\n",
    "\n",
    "    def train_test_split(self,data,test_frac=0.2):\n",
    "        indices = np.arange(len(data))\n",
    "        #np.random.shuffle(indices)\n",
    "        train_indices_rec = int((1 - test_frac)*len(data))\n",
    "        indices_train = indices[:train_indices_rec]\n",
    "        indices_test = indices[train_indices_rec:]\n",
    "        data_train, data_test = data.iloc[indices_train],data.iloc[indices_test]\n",
    "        #print(data_train.head())\n",
    "        #print(data_test.head())\n",
    "        data_train.reset_index(inplace=True)\n",
    "        data_test.reset_index(inplace=True)\n",
    "        data_train.to_csv(r'video_captioning_dataset\\YouTubeClips\\traindata.csv')\n",
    "        data_test.to_csv(r'video_captioning_dataset\\YouTubeClips\\testdata.csv')\n",
    "        \n",
    "        return data_train,data_test\n",
    "\n",
    "    def get_test_data(self,text_path,feat_path):\n",
    "        text_data = pd.read_csv(text_path, sep=',',engine='python')\n",
    "        text_data = text_data[text_data['Language'] == 'English']\n",
    "        text_data['video_path'] = text_data.apply(lambda row: row['VideoID']+'_'+str(int(row['Start']))+'_'+str(int(row['End']))+'.npy', axis=1)\n",
    "        text_data['video_path'] = text_data['video_path'].map(lambda x: os.path.join(feat_path, x))\n",
    "        text_data = text_data[text_data['video_path'].map(lambda x: os.path.exists( x ))]\n",
    "        text_data = text_data[text_data['Description'].map(lambda x: isinstance(x, str))]\n",
    "    \n",
    "        unique_filenames = sorted(text_data['video_path'].unique().tolist())\n",
    "        #unique_filenames = text_data.apply(lambda x:text_data['video_path'].unique())\n",
    "        test_data = text_data[text_data['video_path'].map(lambda x: x in unique_filenames)]\n",
    "        #save data to csv\n",
    "        test_data.to_csv(r'video_captioning_dataset\\YouTubeClips\\alltextdata.csv')\n",
    "        return test_data       \n",
    "        \n",
    "    def create_word_dict(self,sentence_iterator, word_count_threshold=5):\n",
    "        \n",
    "        word_counts = {}\n",
    "        sent_cnt = 0\n",
    "        \n",
    "        for sent in sentence_iterator:\n",
    "            sent_cnt += 1\n",
    "            for w in sent.lower().split(' '):\n",
    "               word_counts[w] = word_counts.get(w, 0) + 1\n",
    "        vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "        \n",
    "        idx2word = {}\n",
    "        idx2word[0] = '<pad>'\n",
    "        idx2word[1] = '<bos>'\n",
    "        idx2word[2] = '<eos>'\n",
    "        idx2word[3] = '<unk>'\n",
    "    \n",
    "        word2idx = {}\n",
    "        word2idx['<pad>'] = 0\n",
    "        word2idx['<bos>'] = 1\n",
    "        word2idx['<eos>'] = 2\n",
    "        word2idx['<unk>'] = 3\n",
    "    \n",
    "        for idx, w in enumerate(vocab):\n",
    "            word2idx[w] = idx+4\n",
    "            idx2word[idx+4] = w\n",
    "    \n",
    "        word_counts['<pad>'] = sent_cnt\n",
    "        word_counts['<bos>'] = sent_cnt\n",
    "        word_counts['<eos>'] = sent_cnt\n",
    "        word_counts['<unk>'] = sent_cnt\n",
    "    \n",
    "        return word2idx,idx2word\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "        print(self.train_text_path)\n",
    "        print(self.train_feat_path)\n",
    " \n",
    "        data = self.get_data(self.train_text_path,self.train_feat_path)\n",
    "        \n",
    "        self.train_data,self.test_data = self.train_test_split(data,test_frac=0.2)\n",
    "        self.train_data.to_csv(f'{self.path_prj}/train.csv',index=False)\n",
    "        self.test_data.to_csv(f'{self.path_prj}/test.csv',index=False)\n",
    "\n",
    "        print(f'Processed train file written to {self.path_prj}/train_corpus.csv')\n",
    "        print(f'Processed test file written to {self.path_prj}/test_corpus.csv')\n",
    "                \n",
    "\n",
    "        train_captions = self.train_data['Description'].values\n",
    "        test_captions = self.test_data['Description'].values\n",
    "    \n",
    "        captions_list = list(train_captions) \n",
    "        captions = np.asarray(captions_list, dtype=np.object)\n",
    "        print(\"captions\",captions)\n",
    "        captions = list(map(lambda x: x.replace('.', ''), captions))\n",
    "        captions = list(map(lambda x: x.replace(',', ''), captions))\n",
    "        captions = list(map(lambda x: x.replace('\"', ''), captions))\n",
    "        captions = list(map(lambda x: x.replace('\\n', ''), captions))\n",
    "        captions = list(map(lambda x: x.replace('?', ''), captions))\n",
    "        captions = list(map(lambda x: x.replace('!', ''), captions))\n",
    "        captions = list(map(lambda x: x.replace('\\\\', ''), captions))\n",
    "        captions = list(map(lambda x: x.replace('/', ''), captions))\n",
    "        print(\"captions_cleaned\",captions)\n",
    "        self.word2idx,self.idx2word = self.create_word_dict(captions, word_count_threshold=0)\n",
    "        \n",
    "        np.save(self.path_prj/ \"word2idx\",self.word2idx)\n",
    "        np.save(self.path_prj/ \"idx2word\" ,self.idx2word)\n",
    "        self.n_words = len(self.word2idx)\n",
    "        #print(len(self.word2idx))\n",
    "    \n",
    "        tf_loss,tf_video,tf_video_mask,tf_caption,tf_caption_mask,tf_probs,train_op= self.build_model()\n",
    "        print(\"tf_video\",tf_video)\n",
    "        print(\"tf_video_mask\",tf_video_mask)\n",
    "        print(\"tf_caption\",tf_caption)\n",
    "        print(\"tf_caption_mask\",tf_caption_mask)\n",
    "        #sess_count=self.tf.compat.v1.InteractiveSession._active_session_count\n",
    "        #if sess_count>0:\n",
    "          #  print(sess_count)\n",
    "           # self.tf.compat.v1.InteractiveSession.close()\n",
    "        sess = tf.compat.v1.InteractiveSession()\n",
    "        tf.compat.v1.global_variables_initializer().run()\n",
    "        #saver = tf.compat.v1.train.Saver(max_to_keep=100, write_version=1)\n",
    "        saver = tf.compat.v1.train.Saver(max_to_keep=100)\n",
    "        var_name_list=[v.name for v in tf.compat.v1.global_variables()]\n",
    "        #print(var_name_list)\n",
    "        \n",
    "    \n",
    "    \n",
    "        loss_out = open('loss.txt', 'w')\n",
    "        val_loss = []\n",
    "    \n",
    "        for epoch in range(0,self.epochs):\n",
    "            val_loss_epoch = []\n",
    "    \n",
    "            index = np.arange(len(self.train_data))\n",
    "\n",
    "            self.train_data.reset_index()\n",
    "            np.random.shuffle(index)\n",
    "            self.train_data = self.train_data.loc[index]\n",
    "    \n",
    "            current_train_data = self.train_data.groupby(['video_path']).first().reset_index()\n",
    "            print(\"current_train_data:\",current_train_data)\n",
    "    \n",
    "            for start, end in zip(\n",
    "                    range(0, len(current_train_data),self.batch_size),\n",
    "                    range(self.batch_size,len(current_train_data),self.batch_size)):\n",
    "    \n",
    "                start_time = time.time()\n",
    "    \n",
    "                current_batch = current_train_data[start:end]\n",
    "                current_videos = current_batch['video_path'].values\n",
    "    \n",
    "                current_feats = np.zeros((self.batch_size, self.video_lstm_step,self.dim_image))\n",
    "                #print(self.video_lstm_step)\n",
    "                #print(self.dim_image)\n",
    "                current_feats_vals = list(map(lambda vid: np.load(vid),current_videos))\n",
    "                #print(\"current_feats_vals\",current_feats_vals[0:5])\n",
    "                current_feats_vals = np.array(current_feats_vals) \n",
    "    \n",
    "                current_video_masks = np.zeros((self.batch_size,self.video_lstm_step))\n",
    "    \n",
    "                for ind,feat in enumerate(current_feats_vals):\n",
    "                    current_feats[ind][:len(current_feats_vals[ind])] = feat\n",
    "                    current_video_masks[ind][:len(current_feats_vals[ind])] = 1\n",
    "    \n",
    "                current_captions = current_batch['Description'].values\n",
    "                current_captions = list(map(lambda x: '<bos> ' + x, current_captions))\n",
    "                current_captions = list(map(lambda x: x.replace('.', ''), current_captions))\n",
    "                current_captions = list(map(lambda x: x.replace(',', ''), current_captions))\n",
    "                current_captions = list(map(lambda x: x.replace('\"', ''), current_captions))\n",
    "                current_captions = list(map(lambda x: x.replace('\\n', ''), current_captions))\n",
    "                current_captions = list(map(lambda x: x.replace('?', ''), current_captions))\n",
    "                current_captions = list(map(lambda x: x.replace('!', ''), current_captions))\n",
    "                current_captions = list(map(lambda x: x.replace('\\\\', ''), current_captions))\n",
    "                current_captions = list(map(lambda x: x.replace('/', ''), current_captions))\n",
    "                print(\"current_captions\",current_captions)\n",
    "\n",
    "    \n",
    "                for idx, each_cap in enumerate(current_captions):\n",
    "                    word = each_cap.lower().split(' ')\n",
    "                    if len(word) < self.caption_lstm_step:\n",
    "                        current_captions[idx] = current_captions[idx] + ' <eos>'\n",
    "                    else:\n",
    "                        new_word = ''\n",
    "                        for i in range(self.caption_lstm_step-1):\n",
    "                            new_word = new_word + word[i] + ' '\n",
    "                        current_captions[idx] = new_word + '<eos>'\n",
    "    \n",
    "                current_caption_ind = []\n",
    "                for cap in current_captions:\n",
    "                    current_word_ind = []\n",
    "                    for word in cap.lower().split(' '):\n",
    "                        if word in self.word2idx:\n",
    "                            current_word_ind.append(self.word2idx[word])\n",
    "                        else:\n",
    "                            current_word_ind.append(self.word2idx['<unk>'])\n",
    "                    current_caption_ind.append(current_word_ind)\n",
    "    \n",
    "                current_caption_matrix = sequence.pad_sequences(current_caption_ind, padding='post', maxlen=self.caption_lstm_step)\n",
    "                current_caption_matrix = np.hstack( [current_caption_matrix, np.zeros( [len(current_caption_matrix), 1] ) ] ).astype(int)\n",
    "                current_caption_masks = np.zeros( (current_caption_matrix.shape[0], current_caption_matrix.shape[1]) )\n",
    "                nonzeros = np.array( list(map(lambda x: (x != 0).sum() + 1, current_caption_matrix ) ))\n",
    "                print(\"current_caption_ind\",current_caption_ind)\n",
    "                print(\"current_caption_matrix\",current_caption_matrix)\n",
    "                for ind, row in enumerate(current_caption_masks):\n",
    "                    row[:nonzeros[ind]] = 1\n",
    "    \n",
    "                probs_val = sess.run(tf_probs, feed_dict={\n",
    "                    tf_video:current_feats,\n",
    "                    tf_caption: current_caption_matrix\n",
    "                    })\n",
    "                print(\"current video feat\",current_feats)\n",
    "                print(\"current video mask\",current_video_masks)\n",
    "                print(\"current caption\",current_caption_matrix)\n",
    "                print(\"current caption mask\",current_caption_masks)\n",
    "                #print(\"probs_val\",probs_val)\n",
    "                _,loss_val = sess.run(\n",
    "                        [train_op, tf_loss],\n",
    "                        feed_dict={\n",
    "                            tf_video: current_feats,\n",
    "                            tf_video_mask : current_video_masks,\n",
    "                            tf_caption: current_caption_matrix,\n",
    "                            tf_caption_mask: current_caption_masks\n",
    "                            })\n",
    "                val_loss_epoch.append(loss_val)\n",
    "    \n",
    "                print('Batch starting index: ', start, \" Epoch: \", epoch, \" loss: \", loss_val, ' Elapsed time: ', str((time.time() - start_time)))\n",
    "                loss_out.write('epoch ' + str(epoch) + ' loss ' + str(loss_val) + '\\n')\n",
    "    \n",
    "            # draw loss curve every epoch\n",
    "            print(val_loss_epoch)\n",
    "            val_loss.append(np.mean(val_loss_epoch))\n",
    "            #val_loss.append(val_loss_epoch)\n",
    "            plt_save_dir = self.path_prj / \"loss_imgs\"\n",
    "            print(plt_save_dir)\n",
    "            print(val_loss)\n",
    "            \n",
    "            plt_save_img_name = 'epoch '+ str(epoch) + '.png'\n",
    "            print(plt_save_img_name)\n",
    "            plt.plot(range(len(val_loss)),val_loss, color='g')\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(plt_save_dir, plt_save_img_name))\n",
    "    \n",
    "            if np.mod(epoch,9) == 0:\n",
    "                print (\"Epoch \", epoch, \" is done. Saving the model ...\")\n",
    "                saver.save(sess, os.path.join(self.path_prj, 'trainmodel'), global_step=epoch)\n",
    "    \n",
    "        loss_out.close()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def inference(self):\n",
    "\n",
    "        self.test_data = self.get_test_data(self.test_text_path,self.test_feat_path)\n",
    "        #test_text_data = pd.read_csv(self.test_text_path, sep=',',engine='python')\n",
    "        #print(test_text_data.shape)\n",
    "        test_videos = self.test_data['video_path'].unique()\n",
    "        print(test_videos)\n",
    "    \n",
    "        self.idx2word = pd.Series(np.load(self.path_prj / \"idx2word.npy\",allow_pickle=True).tolist())\n",
    "    \n",
    "        self.n_words = len(self.idx2word)\n",
    "        #print(\"number of words:\",self.n_words)\n",
    "        video_tf, video_mask_tf, caption_tf, probs_tf, last_embed_tf = self.build_generator()\n",
    "    \n",
    "        sess = tf.compat.v1.InteractiveSession()\n",
    "        #tf.compat.v1.global_variables_initializer().run()\n",
    "        #print(self.model_path)\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "      \n",
    "\n",
    "        #print(saver)\n",
    "        \n",
    "        mypath=os.path.join(self.path_prj,'trainmodel-99')\n",
    "        \n",
    "        \n",
    "        saver.restore(sess,mypath)\n",
    "                \n",
    "    \n",
    "        f = open(f'{self.path_prj}/video_captioning_results.txt', 'w')\n",
    "        for idx, video_feat_path in enumerate(test_videos):\n",
    "            video_feat = np.load(video_feat_path)[None,...]#adding one more dimension to the tensor\n",
    "            print(video_feat)\n",
    "            print(idx)\n",
    "            print(video_feat_path)\n",
    "            print(video_feat.shape)\n",
    "            print(video_feat.shape[1])\n",
    "            print(self.frame_step)\n",
    "            if video_feat.shape[1] == self.frame_step:\n",
    "                video_mask = np.ones((video_feat.shape[0], video_feat.shape[1]))\n",
    "            else:\n",
    "                continue\n",
    "            #print(caption_tf)\n",
    "            #print(video_tf)\n",
    "            #print(video_feat)\n",
    "            #print(video_mask_tf)\n",
    "            gen_word_idx = sess.run(caption_tf, feed_dict={video_tf:video_feat, video_mask_tf:video_mask})\n",
    "            #print(\"wordindex:\",gen_word_idx)\n",
    "            gen_words = self.idx2word[gen_word_idx]\n",
    "            #print(gen_words)\n",
    "            punct = np.argmax(np.array(gen_words) == '<eos>') + 1\n",
    "            #print(\"punct:\",punct)\n",
    "            gen_words = gen_words[:punct]\n",
    "            #print(gen_words)\n",
    "            #print(\"gen_words from list:\",gen_words)\n",
    "            gen_sent = ' '.join(gen_words)\n",
    "            gen_sent = gen_sent.replace('<bos> ', '')\n",
    "            gen_sent = gen_sent.replace(' <eos>', '')\n",
    "            #print(f'Video path {video_feat_path} : Generated Caption {gen_sent}')\n",
    "            print(gen_sent,'\\n')\n",
    "            f.write(video_feat_path + '\\n')\n",
    "            f.write(video_feat_path +','+gen_sent)\n",
    "            f.write(gen_sent + '\\n\\n')\n",
    "        #sess.close()\n",
    "    def process_main(self):\n",
    "        if self.mode == 'train':\n",
    "            self.train()\n",
    "        else:\n",
    "            self.inference()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "#if __name__ == '__main__':\n",
    "    #with ElapsedTimer('Video Captioning'):\n",
    "    \n",
    "       # fire.Fire(VideoCaptioning)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['video_captioning_dataset\\\\YouTubeClips\\\\Iteration_Resnet\\\\Resnet_feat_dir\\\\JM4913Fe-ic_4_15.npy']\n",
      "WARNING:tensorflow:<tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl.BasicLSTMCell object at 0x000001B4C3EAC3D0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl.BasicLSTMCell object at 0x000001B4CA1D2790>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "INFO:tensorflow:Restoring parameters from video_captioning_dataset\\YouTubeClips\\Iteration_Resnet\\trainmodel-99\n",
      "video_captioning_dataset\\YouTubeClips\\Iteration_Resnet\\JM4913Fe-ic_4_15.npy\n",
      "[[[0.09280623 0.31075805 0.06499863 ... 0.7313846  0.27574942 0.25908712]\n",
      "  [0.09280623 0.31075805 0.06499863 ... 0.7313846  0.27574942 0.25908712]\n",
      "  [0.09280623 0.31075805 0.06499863 ... 0.7313846  0.27574942 0.25908712]\n",
      "  ...\n",
      "  [0.03456631 0.3627346  0.02958538 ... 0.26394218 0.02721932 0.23334727]\n",
      "  [0.03456631 0.3627346  0.02958538 ... 0.26394218 0.02721932 0.23334727]\n",
      "  [0.03456631 0.3627346  0.02958538 ... 0.26394218 0.02721932 0.23334727]]]\n",
      "0\n",
      "video_captioning_dataset\\YouTubeClips\\Iteration_Resnet\\Resnet_feat_dir\\JM4913Fe-ic_4_15.npy\n",
      "(1, 80, 2048)\n",
      "80\n",
      "80\n",
      "a little boy is singing in a bathtub \n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_prj='video_captioning_dataset\\YouTubeClips\\Iteration_Resnet'\n",
    "#caption_file='video_corpus.csv'\n",
    "feat_dir='Resnet_feat_dir'\n",
    "#feat_path='video_captioning_dataset\\YouTubeClips\\iteration4\\\\dest'\n",
    "#text_path='video_captioning_dataset\\YouTubeClips\\iteration4\\\\video_corpus.csv'\n",
    "caption_file='test2.csv'\n",
    "mytest=VideoCaptioning(path_prj,caption_file,feat_dir)\n",
    "#mydata=mytest.get_data(text_path,feat_path)\n",
    "#mydata_split=mytest.train_test_split(mydata,test_frac=0.2)\n",
    "#mytest_testdata=mytest.get_test_data(text_path,feat_path)\n",
    "#my_train=mytest.train()\n",
    "#len(mytest_testdata)\n",
    "myinfer=mytest.inference()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
